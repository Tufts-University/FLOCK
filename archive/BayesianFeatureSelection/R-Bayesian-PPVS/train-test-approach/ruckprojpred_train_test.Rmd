---
title: "STX Score Ruck Variable Selection with PPVS - Train/Test Methodology"
author: "Aaron Gardony"
date: "10/17/2023"
output: pdf_document
classoption: landscape
editor_options:
  chunk_output_type: inline
---

# Setup

## Read in libraries

```{r import libraries, message=FALSE, warning=FALSE}
renv::activate()
library(rmarkdown)
library(data.table)
library(ggplot2)
library(brms)
library(emmeans)
library(lme4)
library(tictoc)
library(mosaic)
library(priorsense)
library(afex)
library(kableExtra)
library(viridis)
library(tidyverse)
library(modelr)
library(bayestestR)
library(effectsize)
library(tidybayes)
library(gridExtra)
library(projpred)
library(showtext)
library(tinytex)
library(recipes)
library(extrasteps)# for robust scale equivalent
# devtools::install_github("EmilHvitfeldt/extrasteps")
library(tictoc)
library(brmstools)
# devtools::install_github("mvuorre/brmstools")
```
\newpage

## Read in ruck data    

```{r read data and set up, message=FALSE, warning=FALSE}
ruck_data <- fread("data/AllRuckFeaturesTrainTest.csv", header=TRUE)
ruck_data <- ruck_data %>% filter(Platoon_Squad != "PLT6SQ1")
ruck_data <- ruck_data %>%
  remove_rownames() %>% 
  column_to_rownames(var="Platoon_Squad")

# make training and test datasets
ruck_data_train <- ruck_data %>% filter(Train_Test == "Train") %>% select(-c('Train_Test','recon_score','raid_score'))
ruck_data_test <- ruck_data %>% filter(Train_Test == "Test") %>% select(-c('Train_Test','recon_score','raid_score'))

# perform robust scaling mimicing scikit RobustScaler, separately on the train set
rec_train <- recipe(stx_score ~., data = ruck_data_train) %>% 
  step_robust(all_numeric_predictors()) %>% 
  prep()

rec_train %>% 
  bake(new_data = NULL) -> ruck_data_std_train

# apply training data set robust scaling to the test set
rec_train %>% 
  bake(new_data = ruck_data_test) -> ruck_data_std_test

showtext_auto()
set.seed(8675309) # set random seed for reproducible results
ruck_data %>% ggplot(aes(x = stx_score, col = Train_Test, fill = Train_Test)) + geom_halfeyeh(alpha = 0.5)
load("ruckprojpred_train_test_Env.RData") # uncomment when knitting
```
\newpage

# Variable Selection

For more details see: <https://mc-stan.org/projpred/articles/projpred.html>

In projpred, projection predictive variable selection consists of a search part and an evaluation part. The search part determines the solution path, i.e., the best submodel for each submodel size (number of predictor terms). The evaluation part determines the predictive performance of the submodels along the solution path.

## Fit Maximal (Reference Model)

```{r projpred computation, eval=FALSE, echo=TRUE}
# run maximal reference model, this model includes all predictors and 2-way interactions
# set a shrinkage priors, like the horseshoe or R2D2 priors

tic()
ref_fit <- brm(stx_score ~ (.),
               family = gaussian(),
               data = ruck_data_std_train,
               prior = set_prior(R2D2()), # set a shrinkage prior, like horseshoe or R2D2
               backend = "cmdstanr",
               seed = 8675309,
               refresh = 0,
               iter = 4000,
               cores = 4,
               chains = 4,
               control = list(adapt_delta = 0.99999))
toc()
pp_check(ref_fit, type = "dens_overlay", ndraws = 100)
summary(ref_fit)
bayesplot::mcmc_intervals(ref_fit, pars = vars(starts_with("b_") & !ends_with("Intercept")))
```

## Preliminary Run

```{r prelim run, eval=FALSE, echo=TRUE}

# You can set validate_search to FALSE at first to obtain rough preliminary results. projpred is computationally
# expensive and this makes the first step much faster but does risk overfitting in the selection of the submodel
# size. If possible (in terms of computation time), it is recommended to use the default of
# validate_search = TRUE.

# preliminary run to determine nterms_max
cvvs_valsearchF = projpred::cv_varsel(
  ref_fit,
  method = "forward",
  validate_search = FALSE,
  seed = 8675309
)
```
\newpage

## Plot preliminary run
```{r plot prelim run}
plot(cvvs_valsearchF, stats = c('elpd', 'rmse', "mlpd"), deltas=TRUE, ranking_nterms_max = NA)
```

## Plot search run

```{r search run, eval=FALSE, echo=TRUE}

# run again with default of validate_search = TRUE and with nterms_max = 10

cvvs <- cv_varsel(
  ref_fit,
  method = "forward",
  nterms_max = 10,
  seed = 8675309
)
```
\newpage

## Identify submodel

```{r identify submodel}
plot(cvvs, stats = c('elpd', 'rmse', "mlpd"), deltas=TRUE, ranking_nterms_max = NA)

suggest_size(cvvs) # this heuristic does not work here
modsize_decided = 2

soltrms <- solution_terms(cvvs)
soltrms_final <- head(soltrms, modsize_decided)
soltrms_final
```
\newpage

## Post-selection inference (Experimental)

```{r post-selection inference, echo=TRUE}

# For post-selection inference, project the reference model onto the selected submodel (again)
# I don't really understand what I'm doing here, just following along with this video
# https://www.youtube.com/watch?v=P7bnEPkkTYw

prj <- projpred::project(ref_fit, solution_terms = soltrms_final)
prj_mat <- as.matrix(prj)
library(posterior)
prj_draws <- as_draws_matrix(prj_mat)
tails <- \(x) quantile(x, probs = c(0.025, 0.975))
prj_smmry <- summarize_draws(prj_draws, "median", "mad", "tails")
print(as.data.frame(prj_smmry), digits = 2)
library(bayesplot)
mcmc_intervals(prj_mat)
prj_pppc <- proj_predict(prj, seed = 8675309)
ppc_dens_overlay(y = ruck_data_std_train$stx_score, yrep = prj_pppc)
```
\newpage

## Compute best fit 2-term submodel and evaluate its performance

```{r compute best fit model, echo=TRUE}
# 2 terms
# [1] "vel_ent_whole"        "doct_vel_ratio_whole"
tic()
prior <- c(set_prior("normal(0,100)", class = "b"))
best_fit2 <- brm(stx_score ~ vel_ent_whole + doct_vel_ratio_whole,
               family = "gaussian",
               data = ruck_data_std_train,
               prior=prior, # weakly informative priors
               backend = "cmdstanr",
               seed = 8675309,
               iter = 4000,
               cores = 4,
               chains = 4,
               control = list(adapt_delta = 0.99999))
toc()
summary(best_fit2)
pp_check(best_fit2, type = "dens_overlay", ndraws = 100)
pp_train <- predict(best_fit2)
avgError_train = mean(abs(ruck_data_std_train$stx_score - pp_train[,1]))

pp_test <- predict(best_fit2, newdata = ruck_data_std_test)
avgError_test = mean(abs(ruck_data_std_test$stx_score - pp_test[,1]))

predictVsObs_train = bind_cols(ruck_data_std_train$stx_score, pp_train[,1])
predictVsObs_test = bind_cols(ruck_data_std_test$stx_score, pp_test[,1])
library(ggpubr)
ggplot(predictVsObs_train, aes(x = ...1, y = ...2)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) +
  labs(title = "Bayesian Projection Predictive Variable Selection with LOO-CV (TRAIN)",
       subtitle = "stx_score ~ vel_ent_whole + doct_vel_ratio_whole",
       x = "stx_score", y = "pred stx_score") + 
  lims(x = c(-10,100), y = c(-10,100))+
  annotate(geom="text", x=25, y=60, label=paste("avg error: ",round(avgError_train,2),"%",sep=""), color="red")

ggplot(predictVsObs_test, aes(x = ...1, y = ...2)) +
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) +
  labs(title = "Bayesian Projection Predictive Variable Selection with LOO-CV (TEST)",
       subtitle = "stx_score ~ vel_ent_whole + doct_vel_ratio_whole",
       x = "stx_score", y = "pred stx_score") + 
  lims(x = c(0,100), y = c(0,100))+
  annotate(geom="text", x=25, y=60, label=paste("avg error: ",round(avgError_test,2),"%",sep=""), color="red")

bayesplot::mcmc_intervals(best_fit2, pars = vars(starts_with("b_") & !ends_with("Intercept")))
```
\newpage

# Session Info

```{r session info}
library(sessioninfo)
sessioninfo::session_info(to_file=TRUE)
print(cmdstanr::cmdstan_version(error_on_NA = FALSE))
```
